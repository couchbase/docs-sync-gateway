= High-Availability Replication
:page-partial:
:page-layout: article
:page-status: {release-status-sgw} -- {release-comments-sgw}
:page-edition: enterprise
:page-role:
:page-content-type: conceptual
:description: High availability and Inter-sync-gateway replication (Sync Gateway to Sync Gateway)
:keywords: replication edge-to-cloud sync 'high availability' edge nosql api synchronization replication

include::partial$_std-hdr-sgw.adoc[]
include::partial$block-authors-notes.adoc[tag=wip]
:topic-group: Inter-sync-gateway Replication
:param-related: {xref-sgw-pg-config-properties} | {xref-sgw-pg-rest-api-admin}
:param-abstract: This content provides an overview of Inter-sync-gateway replication high-availability
include::partial$block-abstract.adoc[]

[CAUTION]
include::partial$block-caveats.adoc[tag=enterprise-only]

// End of Page Definition Statements

// include::shared-mobile::partial$_attributes-shared.adoc[]
// include::partial$_attributes-local.adoc[]
// :xref-pfx-sgw: {xref-pfx-sgw}:
// include::partial$_page-index.adoc[]

ifeval::["{releaseStatus}" == "gamma"]
[.pane__frame--orange]
.Author's Notes
--
Document relevant aspects of _High-Availability Replication_.

Information sources include:

* Ticket: https://issues.couchbase.com/browse/DOC-6494[DOC-6494]
* https://docs.google.com/document/d/13E6JOq8u_AaUd_t8FZEPCAuq7jfkjryecjBQBHE3pG8/edit?ts=5e7cd22f#heading=h.dee3943zlt52

.Points to cover include:

* {empty}

Sync Gateway by default must support  High Availability of a replication.
While a  given replication will only run on one node at any given time but when that node fails, the system will automatically elect another node for replication.

Recommendation : To get HA, Users must be configure at least two nodes to run a given replication.

This will apply to Inter-sync-gateway Replication (v2)

Impl note: Based on set of replications defined, system will have a node election process/ heartbeat process to detect if a node is up or not)

Note : This requirement is true only for EE. In case of CE, Sync Gateway will behave as it does today. The replicator will run on the nodes that the users designate in their configuration.

In case of multiple  replications, Sync Gateway by default must distribute the set of replications across all available sync gateway nodes (on which the replications are configured). Even if a replication is configured on multiple nodes, a given replication will only run on one node at any given time.

This will apply to Inter-sync-gateway Replication (v2)

This will provide the following -
Improved throughput by distributing the replications
By ensuring that a replicator only runs on one node even if configured to run on multiple nodes, the system  redundant exchange of data as a result of redundant replications. This will reduce the processing load on the sync gateway and reduce bandwidth usage
This will allow us to deploy a truly homogenous cluster even in SG-replicate based environment where every SGW node in cluster has an identical configuration. This will enable future platform architectural enhancements around mobile manageability .

Impl note: Based on set of replications d

In EE, replications will get automatically get redistributed as more Sync Gateways are added to the cluster or removed from cluster

Expected behavior
https://docs.google.com/document/d/13E6JOq8u_AaUd_t8FZEPCAuq7jfkjryecjBQBHE3pG8/edit#heading=h.vwjngb7h5lpm
--
endif::[]

== About High-Availability Replication

// tag::overview[]
Sync Gateway's inter-sync-gateway replications support High Availability (HA).

In {enterprise} SGR-2 automatically provides for high-availability.
_Node distribution_ ensures all running replications are evenly distributed across all available nodes, regardless of their originating node.

Any given replication runs on only one node at any give time.

When a node fails, the system automatically resumes the node's replications on an alternative node (providing the replication has been configured on multiple nodes).

For {community} deployments::
Although high-availability replication is not automatic, you can make replication availability more robust by configuring or creating the same replication on multiple nodes.
This means that should one node fail the duplicate replication on other nodes may be able to continue.
// end::overview[]

_Action_: To make a replication a candidate for HA the user must configure the same replication on at least two Sync Gateway nodes.

== Node Distribution

The goal of node distribution is to maintain an optimal balance of replications across the cluster.

To achieve this Sync Gateway automatically balances, as equally as possible, the number of replications running on each node.

Where multiple replications are configured on multiple nodes, Sync Gateway automatically distributes these replications across all the available nodes for which the replications are configured.
It continually monitors and redistributes replications as the number of available nodes and the number of running replications in a cluster changes.

The nodes' processing load and bandwidth usage is minimized by ensuring that a replicator runs on only one node at any given time -- even where it has been configured to run on multiple nodes.
This avoids the redundant exchange of data arising from duplicate replication.

// Sync Gateway selects the node to run a replication on is selected by an election process. It uses a heartbeat check to establish whether a node is available or not.
// If it isn't it will try the next node in the configured set.

// Monitoring the node heartbeats is also the trigger for the re-balancing of running replications as it

== Configuration Requirements

To configure replications to be highly available requires that you include their database and replication definitions in the sync gateway configuration on each node within the cluster that you want them to be able to run on.
_At least two nodes are required._

Alternatively, you can use the Admin REST API to initialize the replications on each of the required nodes.

SGW will automatically elect an appropriate node to run them on.
It will redistribute the replication if the node fails, or to maintain an optimal distribution of replications cross available nodes.

{community}::
A above, you need to include your database and replication definitions in the sync gateway configuration on multiple nodes within the cluster.
The replications will run on each of the nodes they are configured on.
No automatic distribution of replications is done.

_Related configuration properties_:  {xref-sgw-pg-config-properties}

== Examples of Expected behavior

This section provides examples of expected behavior in differing scenarios and provides a comparison of how behavior differs between {enterprise} and {community}.


Each scenario involves a sync gateway cluster with multiple nodes.

* Homogenous configuration -- see <<homogenous-config>>
* Homogenous configuration with non-replicating node -- see <<homogenous-non-rep-node>>
* Heterogenous configuration -- see <<hetero-config>>
* Adding more nodes -- see <<adding-more-nodes>>
* Failing node -- see <<failing-node>>

[#homogenous-config]
.Homogenous configuration
=====
.Scenario

* The cluster comprises three SGW nodes
* The same sync gateway configuration is applied across all nodes
* All nodes are configured to run _Replication Id 1_

[{tabs}]
====

{enterprise}::
+
--
* SGW automatically designates one of the three nodes to run _Replication Id 1_.
* If a node goes down, SGW elects one of the remaining nodes to continue _Replication Id 1_.
--

{community}::
+
--
SGW runs _Replication  Id 1_ on all nodes in the cluster.
--
====
=====

[#homogenous-non-rep-node]
.Homogenous configuration with non-replicating node
=====

.Scenario
* The cluster comprises three SGW nodes.
* Each node has the same sync gateway configuration, with one exception. The configuration on _Node 3_ has opted out of replication (`sgreplicate_enabled=false`)
* All Three nodes are configured to run _Replication Id 1_.

[{tabs}]
====
{enterprise}::
+
--
* SGW automatically designates either _Node 1_ or _Node 2_ to run the _Replication Id 1_.
* If either _Node 1_ or _Node 2_ fails, SGW elects the non-failing node.
--

{community}::
+
--
* SGW runs _Replication Id 1_ on all nodes in cluster.

* The system ignores the opt-out flag (`sgreplicate_enabled`).
--
====
=====

[#hetero-config]
.Heterogenous configuration
=====
.Scenario
* The cluster comprises three SGW nodes
* Both _Node 1_ and _Node 2_ are configured to run _Replication Id 1_
* _Node 3_ is configured to run _Replication Id 2_ but *not* _Replication Id 1_

[{tabs}]
====
{enterprise}::
+
--

* SGW automatically distributes _Replication Id 1_ and _Replication Id 2_ so that each runs on *one* of _Node 1_, _Node 2_ or _Node 3_, with no node running both replications simultaneously.
* If any nodes fails whilst running either replication, SGW elects a non-failing node to continue that replication on.
Where two nodes remain the node not running a replication will be chosen.

--

{community}::
+
--
* SGW runs _Replication  Id1_ on _Node 1_ and _Node 2_  in the cluster
* SGW runs _Replication Id 2_ on _Node 3_

Note:

* If _Node 3_ fails, then _Replication Id 2_ will not be continued on either of the remaining nodes as it is not configured on them
* Similarly, if either or both of the other nodes (_Node 1_ and _Node 2_) fails, _Node 3_ will not be a candidate to run the corresponding replication.
--
====
=====

[#adding-more-nodes]
.Adding more nodes
=====

.Scenario
* The cluster comprises a single SGW node
* _Node 1_ is configured to run _Replication Id 1_ and _Replication Id 2_

* LATER . . . _Node 2_ is added to the cluster to run _Replication Id 1_ and _Replication Id 2_.

[{tabs}]
====
{enterprise}::
+
--
* SGW designates _Node 1_ run both _Replication Id 1_ and _Replication Id 2_

* LATER . . . when _Node 2_ is added . . .
** SGW select one of the _Node 1_ replications to run on _Node 2_; let's say it chooses _Replication Id 2_
** SGW stops _Replication Id 2_ on _Node 1_
** SGW starts _Replication Id 2_ on _Node 2_.
--

{community}::
+
--
* SGW designates _Node 1_ to run both _Replication Id 1_ and _Replication Id 2_
* WHEN . . . _Node 2_ is added . . . SGW  designates it to run both _Replication Id 1_ and _Replication Id 2_
--
====
=====

[#failing-node]
.Failing node
=====
.Scenario

* The cluster comprises three SGW nodes with a homogeneous configuration
* All three nodes are configured to run _Replication Id 1_, _Replication Id 2_ and _Replication Id 3_
* LATER . . . _Node 3_ goes down

[{tabs}]
====
{enterprise}::
+
--
SGW automatically distributes the replications, one to each of the nodes
//  to run  _Replication Id 1_, _Id 2_ and _Id 3_ respectively.

* Lets assume the following distribution:
** _Node 1_ runs _Replication Id 1_
** _Node 2_ runs _Replication Id 2_
** _Node 3_ runs _Replication Id 3_

WHEN . . . _Node 3_  goes down . . . SGW elects either _Node 1_ or _Node 2_ to continue running _Replication Id 3_

--

{community}::
+
--
SGW runs all three replications (_Replication Id 1_ , _Replication Id 2_ and _Replication Id 3_) on all three nodes in the cluster (_Node 1_, _Node 2_ and _Node 3_)

WHEN . . . _Node 3_ goes down . . . _Node 1_ and _Node 2_ continue to run _Replication Id 1_, _Replication Id 2_ and _Replication Id 3_
--
====
=====

== Monitoring Node Distribution

Use the __replicationStatus_ endpoint to access information about which replications are running on which nodes -- see: {xref-sgw-ep-admin-api-replication-repstatus}
  |  {xref-sgw-ep-admin-api-replication-repstatus-set}

This information is also collected and available in the log files.



// chrck thie points covered


In clusters containing multiple Sync Gateway nodes, only _one_ of the Sync Gateways should be configured fas the replicating node.

Configuring multiple Sync Gateways as replicating nodes can substantially increase the amount of duplicate work, and therefore should be avoided.

This limitation means the system cannot be guaranteed as Highly Available.
If the replicating Sync Gateway fails or is otherwise removed from the system, then the replications will stop.





include::partial$block-related-content-icr.adoc[]